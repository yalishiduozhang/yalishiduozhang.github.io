<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>向上</title>
      <link href="/2025/01/21/%E5%90%91%E4%B8%8A/"/>
      <url>/2025/01/21/%E5%90%91%E4%B8%8A/</url>
      
        <content type="html"><![CDATA[<p>小芷掰掰手指，心里数了数，自己已经15天没有成功过了。</p><p>太阳扑向操场上女孩子大大的眼，似乎想要把她吞进那红得发紫的日冕层。小芷不服，硬是睁开眼，和它对视。眼睛发酸，不听指令自己锁掉了眼皮。</p><p>“我是怎么了？”小芷不知道。可是她不知道别人可更不知道了。谁会去关心一个闲来无事就躺操场的、安安静静的孩子呢。</p><p>小芷光知道，自己苦苦练半年的排球，却被只练了两个月的好朋友轻轻打败；小芷光知道，自己努力了一学期学的课，却给了她一个不堪入目的成绩；小芷光知道，自己精心写出的歌，却在互联网上无人问津，还被顺带骂了几句。</p><p>小芷捋了捋自己的头发，乱的。</p><p>小芷揉了揉自己的眼睛，酸的。</p><p>小芷擦了擦自己的小鞋子，脏的。</p><p>“幸好妈妈看不到我，不然又要骂我了！”小芷庆幸。“这么不爱干净，一心都在你那奇奇怪怪又多样的爱好上！”母亲熟悉的话冲向小女孩，她赶忙摇头，“不要！不要！”小芷强迫自己不再回忆。自己也已经十七岁了诶。</p><p>操场上的脚步声错综复杂，扰乱着小女孩的思绪。“我也好久没跑了，天天来操场，却不跑步，上次跑步还是一个月前了诶。”小芷侧过身，背对太阳。太阳和小女孩做出了一个梯形的阴影。</p><p>“要不，跑一下？”小芷试探着问自己。</p><p>冲上操场，小芷一直在加速。理智告诉她不能一直这么冲刺，不然跑不了多久。“哼，很快就累了才好，我才不想跑呢！”小芷哼哼。小芷还挖出口袋里的耳机盒，边跑边打开手机，开了音乐。她继续加速，在永远循环无尽的操场上跑着。太阳和小女孩做出了一个圈……</p><p>当小女孩睁开眼睛的时候，天空已经是橙红色的了。太阳它只顾向下走，不曾回头。“嘻，竟然还碰到落日了耶！”小芷笑了。斗气的她才跑了一圈就已精疲力尽，扑向那太阳路过时捂的暖暖的草地。</p><p>小女孩看着太阳，得意，“我现在就这么看着你！眼睛都不带眨呢！”</p><p>可是太阳，只顾向下走。</p><p>小女孩回去的路上，落日把她的身子映照得好长好长，像是她走过的路。那影子一直向前延伸着，仿佛在指引她，所有的跌倒都是向上的伏笔。</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 短篇小说 </tag>
            
            <tag> 创作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是CS224W的相关笔记与解题思路</title>
      <link href="/2025/01/21/CS224W%E8%A7%A3%E9%A2%98/"/>
      <url>/2025/01/21/CS224W%E8%A7%A3%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="这是CS224W的相关笔记与解题思路"><a href="#这是CS224W的相关笔记与解题思路" class="headerlink" title="这是CS224W的相关笔记与解题思路"></a>这是CS224W的相关笔记与解题思路</h2><p>我们接下来的系列将会围绕此展开。</p>]]></content>
      
      
      <categories>
          
          <category> 图神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图神经网络 </tag>
            
            <tag> 解题思路 </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎来到亚里士AI说</title>
      <link href="/2025/01/20/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E4%BA%9A%E9%87%8C%E5%A3%ABAI%E8%AF%B4/"/>
      <url>/2025/01/20/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E4%BA%9A%E9%87%8C%E5%A3%ABAI%E8%AF%B4/</url>
      
        <content type="html"><![CDATA[<h2 id="一页书，敬自己，敬过往，敬你"><a href="#一页书，敬自己，敬过往，敬你" class="headerlink" title="一页书，敬自己，敬过往，敬你"></a>一页书，敬自己，敬过往，敬你</h2><p>朋友，也许你正在网络搜索，无意间发现了我们的网站，也许你正处迷茫，不知道如何继续自己的生活。</p><blockquote><p>停下来，喝杯茶，我们一起聊聊，感悟人生，聊天聊地。</p></blockquote><hr><h2 id="关于这个网站"><a href="#关于这个网站" class="headerlink" title="关于这个网站"></a>关于这个网站</h2><p>这里有AI公众号 <strong>亚里士AI说</strong> 精神的传承，本人也是该公众号作者，我会在这里分享:</p><ul><li>莫名其妙的心理感悟</li><li>完整细腻的AI理论</li><li>突发奇想的文学创作</li><li>And so on…</li></ul><h2 id="作者是谁？"><a href="#作者是谁？" class="headerlink" title="作者是谁？"></a>作者是谁？</h2><p>正如网站显示的，作者是亚里士多章，目前正在大学攻读AI领域。</p><h3 id="大学所得成就"><a href="#大学所得成就" class="headerlink" title="大学所得成就"></a>大学所得成就</h3><ul><li>AI专业排名 <strong>Rank 1</strong></li><li>数学建模领域深耕，获奖有：<ul><li>“高教社杯”全国大学生数学建模竞赛 <strong>国家二等奖</strong></li><li>“电工杯”数学建模竞赛 <strong>国家一等奖</strong></li><li>本校数学建模竞赛一等奖</li></ul></li><li>还在探索的：<ul><li>asc竞赛</li><li>算法竞赛</li><li>数据挖掘</li><li>童话创作</li></ul></li></ul><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><blockquote><p>千里之行，始于足下</p></blockquote><p>这是一个起点，希望能在这里:</p><ul><li>见证我们的成长</li><li>记录我们的故事</li><li>存档我们的知识</li></ul><p>欢迎常来做客，让我们一起探索AI的奇妙世界~</p><hr><p>如果你也对AI感兴趣，欢迎:</p><ul><li>收藏本站常来看看</li><li>一起探讨技术与生活</li></ul><p>期待与你相遇！ ❤️</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 闲言碎语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监督学习模型：从输入到输出的映射</title>
      <link href="/2025/01/20/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
      <url>/2025/01/20/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="监督学习模型的定义"><a href="#监督学习模型的定义" class="headerlink" title="监督学习模型的定义"></a>监督学习模型的定义</h2><p>监督学习模型就是将一个或多个输入转化为一个或多个输出的方式。比如，我们可以将某部二手丰田普锐斯的车龄和行驶里程作为输入，预估的车辆价格则是输出。</p><p>相当于改变参数，然后可以形成一个正确函数进行预测。</p><div class="note success flat"><p>举例：easy-tensorflow中实现的神经网络训练底层就是这个原理，结果一定数量训练将随机生成的梯度往正确梯度上改变，最后实现正确预测。</p></div><h2 id="生成模型与判别模型的区别"><a href="#生成模型与判别模型的区别" class="headerlink" title="生成模型与判别模型的区别"></a>生成模型与判别模型的区别</h2><details><summary>生成 (Generative) 模型与判别 (Discriminative) 模型</summary><ul><li>判别模型就像一个熟练的图片分类员，看到图片就能快速判断</li><li>生成模型就像一个画家，了解狗的特征，可以画出各种狗的图片</li></ul><p>模型 <em>y</em> &#x3D; <em>f</em>[<em>x, ϕ</em>] 属于判别模型。这类模型基于实际测量的数据 x 来预测输出 y。另一种方法是构建生成模型 <em>x</em> &#x3D; <em>g</em>[<em>y, ϕ</em>]，在这种模型中，实际测量的数据 x 被看作是输出 y 的函数。</p><p>虽然生成模型的缺点是它们不直接预测 y，但它们的优势在于能够融入关于数据生成方式的先验知识。比如，如果我们要预测图像 x 中汽车的三维位置和方向 y，我们可以在函数 <em>x</em> &#x3D; <em>g</em>[<em>y, ϕ</em>] 中加入关于汽车形状、三维几何和光传输的知识。</p><p>尽管这听起来是个好主意，但实际上，在现代机器学习中，判别模型更为主流。这是因为在生成模型中利用先验知识所带来的优势通常不及利用大量训练数据来学习灵活的判别模型所获得的优势。</p></details><h2 id="损失函数与成本函数"><a href="#损失函数与成本函数" class="headerlink" title="损失函数与成本函数"></a>损失函数与成本函数</h2><details><summary>损失函数（Loss Function）与成本函数（Cost Function）</summary><p>在机器学习领域，”损失函数”和”成本函数”这两个术语通常可以互换使用。但更准确地说：</p><ul><li>损失函数是指与单个数据点相关的具体项（例如，方程 2.5 中每个平方项）</li><li>成本函数是指需要被最小化的整体量（即方程 2.5 中的整个右侧部分）</li></ul><p>成本函数可能还包含与单个数据点无关的其他项。更广义上，目标函数指的是任何需要最大化或最小化的函数。</p></details><h2 id="思考题与解答"><a href="#思考题与解答" class="headerlink" title="思考题与解答"></a>思考题与解答</h2><details><summary>例题</summary><p><strong>问题 2.1</strong> 为了在损失函数（方程 2.5）上实现”downhill”，我们需要计算它对参数 <em>ϕ</em>0 和 <em>ϕ</em>1 的梯度。请计算出这两个参数的梯度值 <em>∂L</em>&#x2F;<em>∂ϕ</em>0 和 <em>∂L</em>&#x2F;<em>∂ϕ</em>1 的具体表达式。</p><p><strong>问题 2.2</strong> 请证明我们可以通过将问题 2.1 中的导数设置为零，然后求解 <em>ϕ</em>0 和 <em>ϕ</em>1，以闭合形式找到损失函数的最小值。</p><p><strong>问题 2.3</strong> 考虑将线性回归改造为生成模型，形式为 <em>x</em> &#x3D; <em>g</em>[<em>y, ϕ</em>] &#x3D; <em>ϕ</em>0 + <em>ϕ</em>1<em>y</em>。请问这种情况下的新损失函数是什么？</p></details><details><summary>解答</summary><h3 id="问题-2-1-解答"><a href="#问题-2-1-解答" class="headerlink" title="问题 2.1 解答"></a>问题 2.1 解答</h3><p>损失函数L定义为：<br>L &#x3D; ∑(yi - (φ0 + φ1xi))²</p><ol><li><p>计算∂L&#x2F;∂φ0：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">∂L/∂φ0 = ∂/∂φ0 [∑(yi - (φ0 + φ1xi))²]</span><br><span class="line">= ∑ 2(yi - (φ0 + φ1xi)) * (-1)</span><br><span class="line">= -2∑(yi - φ0 - φ1xi)</span><br></pre></td></tr></table></figure></li><li><p>计算∂L&#x2F;∂φ1：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">∂L/∂φ1 = ∂/∂φ1 [∑(yi - (φ0 + φ1xi))²]</span><br><span class="line">= ∑ 2(yi - (φ0 + φ1xi)) * (-xi)</span><br><span class="line">= -2∑(yi - φ0 - φ1xi)xi</span><br></pre></td></tr></table></figure></li></ol><h3 id="问题-2-2-解答"><a href="#问题-2-2-解答" class="headerlink" title="问题 2.2 解答"></a>问题 2.2 解答</h3><p>要找到损失函数的最小值，我们将导数设为零并解方程：</p><p>φ1 &#x3D; (n∑xiyi - ∑xi∑yi) &#x2F; (n∑xi² - (∑xi)²)<br>φ0 &#x3D; (∑yi - φ1∑xi) &#x2F; n</p><p>这种方法只适用于线性回归，因为它具有特殊的二次形式损失函数。</p><h3 id="问题-2-3-解答"><a href="#问题-2-3-解答" class="headerlink" title="问题 2.3 解答"></a>问题 2.3 解答</h3><ol><li>新的生成模型：x &#x3D; g[y, φ] &#x3D; φ0 + φ1y</li><li>新的损失函数：L &#x3D; ∑(xi - (φ0 + φ1yi))²</li><li>逆函数：y &#x3D; (x - φ0) &#x2F; φ1</li></ol><p>在线性回归的情况下，生成模型和判别模型会得到相同的拟合线，但在更复杂的模型中可能不适用。在实践中，判别模型通常更受欢迎，因为它们直接针对预测任务进行优化。</p></details>]]></content>
      
      
      <categories>
          
          <category> 机器学习基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
