<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CS224W系列 | (2)节点嵌入</title>
      <link href="/2025/02/11/CS224W%E7%B3%BB%E5%88%97-2-%E8%8A%82%E7%82%B9%E5%B5%8C%E5%85%A5/"/>
      <url>/2025/02/11/CS224W%E7%B3%BB%E5%88%97-2-%E8%8A%82%E7%82%B9%E5%B5%8C%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<p>今天我们来探讨图嵌入，我们要用一个指定维度的向量来表示我们的图节点，并且想要保持它作为图中一份子的特征，也就是，相邻的节点的节点嵌入也应该相似。</p><p>接下来我们来详细分析。</p><h2 id="图的传统机器学习方法"><a href="#图的传统机器学习方法" class="headerlink" title="图的传统机器学习方法"></a>图的传统机器学习方法</h2><p><img src="https://pic1.imgdb.cn/item/67aaf0d3d0e0a243d4fe49f1.png" alt="传统图机器学习方法"><br>给定一个输入图，提取节点、连接和图层面的特征，然后训练一个模型，比如支持向量机、神经网络等，来将图特征映射到标签。<br>而我们用到的这些图的特征通常都是人工提取设计的，方法可以参考我们的上一节。</p><h2 id="革新-图表示学习"><a href="#革新-图表示学习" class="headerlink" title="革新-图表示学习"></a>革新-图表示学习</h2><p><img src="https://pic1.imgdb.cn/item/67aaf20dd0e0a243d4fe4a5c.png" alt="图表示学习"><br>如图所示，我们的图表示学习可以正是消除每次都需要进行特征工程需求的方法。</p><h3 id="图表示学习的核心目标"><a href="#图表示学习的核心目标" class="headerlink" title="图表示学习的核心目标"></a>图表示学习的核心目标</h3><p><img src="https://pic1.imgdb.cn/item/67aaf2a4d0e0a243d4fe4a90.png" alt="goal"><br>简单来说，图表示学习的核心目标就是实现高效的、与任务无关的特征学习。</p><p>把图节点嵌入到向量空间中，再利用我们熟悉的向量数据处理方法处理数据。</p><h3 id="为什么要进行图嵌入-Node-Embedding"><a href="#为什么要进行图嵌入-Node-Embedding" class="headerlink" title="为什么要进行图嵌入(Node Embedding)"></a>为什么要进行图嵌入(Node Embedding)</h3><p>我们做的所有处理都是为了方便的机器学习，正由于图结构的特殊性，我们在寻找一个方便的途径可以把图进行表示，而图嵌入正是实现这个途径的方法。</p><p>我们希望用节点间嵌入的相似性来表示它们在网络中的相似程度。</p><p>例如：</p><ul><li>两个节点是邻居（通过一条边连接）</li><li>编码网络信息</li><li>用于多种下游预测任务：<ul><li>节点分类</li><li>边预测</li><li>图分类</li><li>异常节点检测</li><li>聚类</li><li>And so on…<br><img src="https://pic1.imgdb.cn/item/67aaf47dd0e0a243d4fe4bbb.png" alt="下游预测任务"></li></ul></li></ul><h3 id="图嵌入实例"><a href="#图嵌入实例" class="headerlink" title="图嵌入实例"></a>图嵌入实例</h3><p><img src="https://pic1.imgdb.cn/item/67aaf4b8d0e0a243d4fe4bc9.png" alt="node embedding example"><br>上图实现了对Zachary空手道俱乐部网络的2D节点嵌入。</p><h2 id="编码器-Encoder-和解码器-Decoder"><a href="#编码器-Encoder-和解码器-Decoder" class="headerlink" title="编码器(Encoder)和解码器(Decoder)"></a>编码器(Encoder)和解码器(Decoder)</h2><p>接下来我们会探讨图嵌入的具体实现方法。<br>我们先来说说编码器和解码器。</p><p><img src="https://pic1.imgdb.cn/item/67aaf5f4d0e0a243d4fe4c6a.png" alt="简图G"><br>假设我们有一个无向图G，V是节点集合，矩阵A是邻接矩阵。</p><p>我们的目标是对节点进行编码，使得嵌入空间中的相似度（例如点积）能够近似表示图中节点之间的相似性。</p><p><img src="https://pic1.imgdb.cn/item/67aaf670d0e0a243d4fe4c87.png" alt="编码器"></p><blockquote><p>图示说明：<br>左侧：原始网络<br>展示了节点u,v及其在网络中的连接关系<br>右侧：嵌入空间<br>通过编码函数ENC将节点映射为向量zu,zv<br>映射过程：<br>ENC(u)将节点u映射到嵌入空间<br>ENC(v)将节点v映射到嵌入空间</p></blockquote><p><img src="https://pic1.imgdb.cn/item/67aaf6f3d0e0a243d4fe4cc2.png" alt="解码器"></p><h3 id="节点嵌入的学习总过程"><a href="#节点嵌入的学习总过程" class="headerlink" title="节点嵌入的学习总过程"></a>节点嵌入的学习总过程</h3><p>我们来总结一下节点嵌入的过程。</p><ol><li>编码器将节点映射为嵌入向量</li><li>定义节点相似度函数(即在原始网络中衡量节点相似度的方法，如共同邻居、Jaccard相似性、余弦相似性等)</li><li>解码器DEC将嵌入向量的相似度映射为相似性得分</li><li>优化编码器参数，使得原始网络中的相似度similarity(u,v)近似于嵌入空间中的相似度（参见下图，这里是用点积来衡量相似度）。</li></ol><p><img src="https://pic1.imgdb.cn/item/67aaf8a4d0e0a243d4fe4ddd.png" alt="相似度"></p><h3 id="两个关键组件"><a href="#两个关键组件" class="headerlink" title="两个关键组件"></a>两个关键组件</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Encoder作为编码器的功能是将每个图节点映射为d维向量。<br><img src="https://pic1.imgdb.cn/item/67aaf9f2d0e0a243d4fe4e42.png" alt="Encoder"></p><h4 id="Similarity-Function"><a href="#Similarity-Function" class="headerlink" title="Similarity Function"></a>Similarity Function</h4><p>相似度函数定义向量空间与原始网络的关系映射。<br><img src="https://pic1.imgdb.cn/item/67aafa49d0e0a243d4fe4e58.png" alt="相似度函数"></p><h3 id="浅层编码-Shallow-Embedding"><a href="#浅层编码-Shallow-Embedding" class="headerlink" title="浅层编码(Shallow Embedding)"></a>浅层编码(Shallow Embedding)</h3><p>最简单的编码方法：编码器就是一个嵌入查找表</p><p>数学表达如下：<br><img src="https://pic1.imgdb.cn/item/67aafb17d0e0a243d4fe4e6f.png" alt="浅层编码器"></p><p>在浅层编码器里，编码器就只是一个嵌入查找表。<br>如果你好奇嵌入矩阵的结构的话，我们可以看下图：</p><p><img src="https://pic1.imgdb.cn/item/67aafe8bd0e0a243d4fe4f20.png" alt="浅层编码器"></p><p>嵌入矩阵Z的结构：</p><p>矩阵维度：</p><ul><li><strong>行数</strong>: 表示嵌入向量的维度 (Dimension&#x2F;size of embeddings)</li><li><strong>列数</strong>: 等于节点总数 (one column per node)<br>矩阵内容：</li><li><strong>每一列</strong>是一个节点的嵌入向量 (embedding vector for a specific node)</li><li><strong>所有节点的嵌入向量</strong>组成完整的嵌入矩阵 (embedding matrix)</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图机器学习浅入门知识点梳理</title>
      <link href="/2025/02/10/%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%85%E5%85%A5%E9%97%A8%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86/"/>
      <url>/2025/02/10/%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%85%E5%85%A5%E9%97%A8%E7%9F%A5%E8%AF%86%E7%82%B9%E6%A2%B3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文参考自 <a href="https://huggingface.co/blog/zh/intro-graphml">图机器学习入门</a></p></blockquote><p>今天不谈AI具体应用，我们来学一学图机器学习。尝试靠一文，带你入门图机器学习。</p><p>我们首先会学习什么是图，为什么使用图，以及如何最佳地表示图。然后我们简要介绍大家如何在图数据上学习，从神经网络以前的方法到现在广为人知的图神经网络(Graph Neural Network,<strong>GNN</strong>)。最后，我们也将一窥图数据上的<strong>Transformers</strong>世界。</p><h2 id="什么是图？"><a href="#什么是图？" class="headerlink" title="什么是图？"></a>什么是图？</h2><p>本质上讲，图描述着由关系相互链接起来的实体。</p><p>现实中很多图的例子，包括我现在正在做的社交网络（微信，以及任何链接论文和作者的引用网络）、知识图谱（UML图，百科全书，以及那些页面之间有超链接的网站）、被表示成句法树的句子、3D网格，甚至是蜘蛛网。因此，图真的无处不在。</p><p>图（或网络）中的实体称为 <em>节点</em> (或顶点)，它们之间的连接称为 <em>边</em> (或链接)。举个例子，在社交网络中，节点是用户，而边是他（她）们之间的连接关系；在分子中，节点是原子，而边是它们之间的分子键。</p><ul><li><p>可以存在不止一种类型的节点或边的图称为 <strong>异构图</strong> ，比如，引用网络的节点有论文和作者两种类型，含有多种关系类型的XML图的边是多类型地。异构图不仅能由其拓扑结构来表征，它需要额外的信息。本文主要讨论 <strong>同构图</strong> 。</p></li><li><p>图还可以是 <strong>有向</strong> 的，如在一个关注网络中，A关注了B，但B可以不关注A。当然，也有 <strong>无向</strong> 图。如一个分子中，原子间的关系是双向的。边可以连接不同节点，也可以自己连接自己（<strong>自连边</strong>，self-edges），但不是所有的节点都必须有连接。</p></li></ul><p>所以，如果你想使用自己的数据，首先你必须考虑如何最佳地刻画它。同构还是异构？有向还是无向？有环还是无环？</p><h2 id="图有什么用途？"><a href="#图有什么用途？" class="headerlink" title="图有什么用途？"></a>图有什么用途？</h2><p>图的用途主要包括节点层面、边层面和图层面的用途。</p><p>在 <strong>节点层面</strong>，通常用于属性预测或缺失边预测。举个例子，<u>Alphafold</u> 使用节点属性预测方法，在给定分子总体图的条件下预测原子的 3D 坐标，并由此预测分子在 3D 空间中如何折叠，这是个比较难的生物化学问题。</p><p>在 <strong>边层面</strong>，我们可以做边属性预测或缺失边预测。边属性预测可以用于给定药物对（pair）的条件下预测药物的不良副作用。缺失边预测的最经典应用是推荐系统中对两个节点是否相关的预测。</p><p>在 <strong>图层面</strong>，我们有在药物发现任务中用于生成新的可能的药物分子的<u>图生成</u>任务，在物理学中用于预测系统的<u>图演化</u>（给定一个图，预测它会如何随时间演化）任务，也有预测分子毒性的<u>图层面预测</u>任务（基于图的分类或回归任务）。</p><p>当然也有 <strong>子图层面</strong>的工作。我们可用于社区检测和子图属性检测。社交网络用社区检测确定人们之间如何连接。我们可以在行程系统 (如 <u>Google Maps</u>) 中发现子图属性预测的身影，它被用于预测到达时间。</p><p>完成这些任务有两种方式。</p><p>当你想要预测特定图的演化时，你工作在 <strong>直推</strong> （transductive）模式，直推模式中的所有训练、验证和推理都是基于同一张图。<strong>但是！这种方式要多加注意！在同一张图上创建训练&#x2F;评估&#x2F;测试集并不容易。</strong> 然而，很多任务其实是工作在不同的图上的（不同的训练&#x2F;评估&#x2F;测试集划分），我们称之为 <strong>归纳</strong>（inductive）模式。</p><h2 id="如何表示图？"><a href="#如何表示图？" class="headerlink" title="如何表示图？"></a>如何表示图？</h2><p>常用的表示图以用于后续处理和操作的方法有 2 种：</p><ul><li>表示成所有边的集合（很有可能也会加上所有节点的集合用以补充）</li><li>表示成所有节点的邻接矩阵。邻接矩阵是一个 \(node_{size} \times node_{size}\) 大小的方阵，它指明图上哪些节点间是直接相连的 (若 \(n_i\) 和 \(n_j\) 相连则 \(A_{ij} &#x3D; 1\)，否则为 0)。</li></ul><blockquote><p><strong>注意</strong> 多数图的边连接并不稠密，因此它们的邻接矩阵是稀疏的，这个会让计算变得困难。</p></blockquote><p>虽然这些表示看上去熟悉，但是可别被骗了！！！</p><p>图与机器学习中使用的典型对象大不相同，因为它们的拓扑结构比序列（如文本或音频）或有序网格（如图像和视频）复杂得多：即使它们可以被链表或矩阵所表示，但它们并不能被当做有序对象来处理。</p><p>这到底意味着什么呢？</p><p>如果你有一个句子，你交换了这个句子的词序，你就创造了一个新句子。如果你有一张图像，然后你重排了这个图像的列，你就创造了一张新图像。</p><p>但图并不会如此。如果你重排了图的边列表或邻接矩阵的列，图还是同一个图（一个更正式的叫法是 <strong>置换不变性</strong> （permutation invariance））。</p><p><img src="https://pic1.imgdb.cn/item/67a9e57bd0e0a243d4fdfa68.png" alt="置换不变性"></p><p>左图，一个小型图 (黄色是节点，橙色是边)。中图，该图的邻接矩阵，行与列的节点按字母排序可以看到第一行的节点 A，与 E 和 C 相连右图重排后的邻接矩阵 (列不再按字母序排了)，但这还是该图的有效表示：A 节点仍然与 E 和 C 相连。</p><h2 id="基于机器学习的图表示"><a href="#基于机器学习的图表示" class="headerlink" title="基于机器学习的图表示"></a>基于机器学习的图表示</h2><p>使用机器学习处理图的一般流程是：首先为你感兴趣的对象（根据你的任务，可以是节点、边或者是全图）生成一个有意义的表示，然后使用它们训练一个目标任务的预测器。与其他模态数据一样，我们想要对这些对象的数学表示施加一些约束，使得相似的对象在数学上是相近的。然而这种相似性在图机器学习上很难严格定义，举个例子，具有相同标签的两个节点和具有相同邻居的两个节点哪两个更相似？</p><blockquote><p><strong>注意</strong><br> 在随后的部分，我们将聚焦于如何生成节点的表示。一旦你有了节点层面的表示，就有可能获得边或图层面的信息。你可以通过把边所连接的两个节点的表示串联起来或作做一个点积来得到边层面的信息。至于图层面的信息，可以通过对图上所有节点的表示串联起来的张量做一个全局池化（平均，求和等）来获得。当然，这么做会平滑掉或丢失掉整图上的一些信息，使用迭代的分层池化可能更合理，或者增加一个连接到图上所有其他节点的<strong>虚拟节点</strong>，然后使用它的表示作为整图的表示。</p></blockquote><h3 id="神经网络以前的方法"><a href="#神经网络以前的方法" class="headerlink" title="神经网络以前的方法"></a>神经网络以前的方法</h3><h4 id="只使用手工设计特征"><a href="#只使用手工设计特征" class="headerlink" title="只使用手工设计特征"></a>只使用手工设计特征</h4><p>在神经网络出现之前，图以及图中的感兴趣项可以被表示为特征的组合，这些组合是针对特定任务的。尽管现在存在 <u>更复杂的特征生成方法</u>，这些特征仍然被用于<u>数据增强</u>和 <u>半监督学习</u>。这是，你主要的工作就是根据目标任务，找到最佳的用于后续网络训练的特征。这时，你主要的工作是根据目标任务，找到最佳的用于后续网络训练的特征。</p><p><strong>节点层面特征</strong> 可以提供关于其重要性（该节点对于图有多重要）以及结构性信息（节点周围的图形状如何），两者可以结合。</p><p>节点<strong>中心性</strong>(centrality) 度量图中节点的重要性。它可以递归计算，即不断对每个节点的邻节点的中心性求和直到收敛，也可以通过计算节点间的最短距离来获得。节点的<strong>度</strong>(degree)度量节点的直接邻居的数量。<strong>聚类系数</strong>(clustering coefficient)度量一个节点的邻节点之间相互连接的程度（我的两个朋友之间是否也认识？）。<strong>图元度向量</strong>(Graphlets degree vectors,GDV)计算给定根节点的不同图元的数目，这里图元是指给定数目的连通节点可创建的所有迷你图（如：3 个连通节点可以生成一个有两条边的线，或者一个 3 条边的三角形）</p><p><img src="https://pic1.imgdb.cn/item/67a9e5bfd0e0a243d4fdfa82.png" alt="图元"></p><p><strong>边层面特征</strong>带来了关于节点间连通性的更多细节信息，有效地补充了图的表示，有：两节点间的 <strong>最短距离</strong> (shortest distance)，它们的<strong>公共邻居</strong>(common neighbours)，以及它们的<strong>卡兹指数</strong>(Katz index) (表示两节点间从所有长度小于某个值的路径的数目，它可以由邻接矩阵直接算得) 。</p><p><strong>图层面特征</strong>包含了关于图相似性和规格的高层信息。总<strong>图元数</strong>尽管计算上很昂贵，但提供了关于子图形状的信息。<strong>核方法</strong>通过不同的<strong>节点袋</strong>（bag of nodes）（类似于词袋（bag of words））方法度量图之间的相似性。</p><h4 id="基于随机游走-DeepWalk-的方法"><a href="#基于随机游走-DeepWalk-的方法" class="headerlink" title="基于随机游走(DeepWalk)的方法"></a>基于随机游走(DeepWalk)的方法</h4><p><u>随机游走的方法</u>使用在随机游走时从节点j访问节点i的可能性来定义相似矩阵，这些方法结合了局部和全局的信息。举个例子，<u>Node2Vec</u>模拟图中节点的随机游走，把这些游走路径建模乘跳字(skip-gram)，这与我们处理句子中的词很相似，然后计算嵌入。基于随机游走的方法也可被用于<u>加速Page Rank方法</u>，帮助计算每个节点的重要性得分（举个例子，如果重要性得分是基于每个节点与其他节点的连通度的话，我们可以用随机游走访问到每个节点的频率来模拟这个连通度。）</p><p>然而，这些方法的限制也很明显：如果我们加入一个新节点应该怎么办?只能从头再来。另外，我们也不能很好地捕获节点间的结构相似性，也使用不了新加入的特征。</p><h2 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h2><p>神经网络可泛化至未见数据。我们在上文已经提到了一些图表示的约束，那么一个好的神经网络应该有哪些特性？</p><p>它应该：</p><ul><li><p>满足置换不变性:</p><ul><li>等式: \(f(P(G))&#x3D;f(G)\)，这里 \(f\) 是神经网络，\(P\) 是置换函数，\(G\) 是图。</li><li>解释：置换后的图和原图经过同样的神经网络后，其表示应该是相同的。</li></ul></li><li><p>满足置换等价性：</p><ul><li>公式: \(P(f(G))&#x3D;f(P(G))\)，同样 \(f\) 是神经网络，\(P\) 是置换函数，\(G\) 是图。</li><li>解释：先置换图再传给神经网络和对神经网络的输出图表示进行置换是等价的。</li></ul></li></ul><p>典型的神经网络，如循环神经网络（RNN）或卷积神经网络(CNN)并不是置换不变的。因此，<u>图神经网络（GNN）</u>作为新的架构被引入来解决这一问题（最初作为状态机使用）。</p><p>一个GNN由连续的层组成，一个GNN层通过<strong>消息传递</strong>过程把一个节点表示成其邻节点及其自身表示的组合（<strong>聚合</strong>(aggregation)），然后我们还会使用一个激活函数(如sigmoid)去增加一些非线性。</p><blockquote><p><strong>与其他模型相比</strong><br>CNN可以看作一个领域（即滑动窗口）大小和顺序固定的GNN，也就是说CNN不是置换等价的。一个没有位置嵌入(positional embedding)的<u>Transformer</u>模型可以被看作一个工作在全连接的输入图上的GNN。</p></blockquote><h3 id="聚合和消息传递"><a href="#聚合和消息传递" class="headerlink" title="聚合和消息传递"></a>聚合和消息传递</h3><p>多种方式可用于聚合邻节点的消息，举例说，有求和、取平均等。一些值得关注的工作有：</p><ul><li><p><u>图卷积网络</u>对目标节点的所有邻节点的归一化表示取平均来做聚合（大多数 GNN 其实是 GCN）；</p></li><li><p><u>图注意力网络</u> 会学习如何根据邻节点的重要性不同来加权聚合邻节点（与 transformer 模型想法相似）；</p></li><li><p><u>GraphSAGE</u> 先在不同的跳数上进行邻节点采样，然后基于采样的子图分多步用最大池化（max pooling）方法聚合信息；</p></li><li><p><u>图同构网络</u> 先计算对邻节点的表示求和，然后再送入一个 MLP 来计算最终的聚合信息。</p></li></ul><blockquote><p><strong>选择聚合方法</strong><br>一些聚合技术 (尤其是均值池化和最大池化) 在遇到在邻节点上仅有些微差别的相似节点的情况下可能会失败 (举个例子：采用均值池化，一个节点有 4 个邻节点，分别表示为 1，1，-1，-1，取均值后变成 0；而另一个节点有 3 个邻节点，分别表示为 - 1，0，1，取均值后也是 0。两者就无法区分了。) 。</p></blockquote><h3 id="GNN的形状和过平滑问题"><a href="#GNN的形状和过平滑问题" class="headerlink" title="GNN的形状和过平滑问题"></a>GNN的形状和过平滑问题</h3><p>每加一个新层，节点表示中就会包含越来越多的节点信息。</p><p>一个节点，在第一层，只会聚合它的直接邻节点的信息。到第二层，它们仍然只聚合直接邻节点信息，但这次，他们的直接邻节点的表示已经包含了它们各自的邻节点信息 (从第一层获得) 。经过 n 层后，所有节点的表示变成了它们距离为 n 的所有邻节点的聚合。如果全图的直径小于 n 的话，就是聚合了全图的信息！</p><p>如果你的网络层数过多，就有每个节点都聚合了全图所有节点信息的风险 (并且所有节点的表示都收敛至相同的值) ，这被称为 <strong>过平滑问题</strong>(the oversmoothing problem)。</p><p>这可以通过如下方式来解决：</p><ul><li>设计GNN层数时，要首先分析图的直径和形状，层数不能过大，以确保每个节点不聚合全图的信息</li><li>增加层的复杂性</li><li>增加非消息传递层来处理消息（如简单的MLP层）</li><li>增加跳跃连接(skip-connections)</li></ul><p>过平滑问题是图机器学习的重要研究领域，因为它阻止了GNN的变大，而在其他模态数据上Transformer之类的模型已经证明了把模型变大是有很好的效果的。</p><h2 id="图Transformers"><a href="#图Transformers" class="headerlink" title="图Transformers"></a>图Transformers</h2><p>没有位置嵌入 (positional encoding) 层的 Transformer 模型是置换不变的，再加上 Transformer 模型已被证明扩展性很好，因此最近大家开始看如何改造 Transformer 使之适应图数据 (综述) 。多数方法聚焦于如何最佳表示图，如找到最好的特征、最好的表示位置信息的方法以及如何改变注意力以适应这一新的数据。</p><p>这里我们收集了一些有意思的工作，这些工作在现有的最难的测试基准之一斯坦福开放图测试基准（<u>Open Graph Benchmark, OGB</u>）上取得了最高水平或接近最高水平的结果：</p><ul><li><p><u>Graph Transformer for Graph-to-Sequence Learning</u> (Cai and Lam, 2020) 介绍了一个图编码器，它把节点表示为它本身的嵌入和位置嵌入的级联，节点间关系表示为它们间的最短路径，然后用一个关系增强的自注意力机制把两者结合起来。</p></li><li><p><u>Rethinking Graph Transformers with Spectral Attention</u> (Kreuzer et al, 2021) 介绍了谱注意力网络 (Spectral Attention Networks, SANs)。它把节点特征和学习到的位置编码（从拉普拉斯特征值和特征向量中计算得到）结合起来，把这些作为注意力的键（keys）和查询（queries），然后把边特征作为注意力的值（values）。</p></li><li><p><u>GRPE: Relative Positional Encoding for Graph Transformer</u> (Park et al, 2021) 介绍了图相对位置编码 Transformer。它先在图层面的位置编码中结合节点信息，在边层面的位置编码中也结合节点信息，然后在注意力机制中进一步把两者结合起来。</p></li><li><p><u>Global Self-Attention as a Replacement for Graph Convolution</u> (Hussain et al, 2021) 介绍了边增强 Transformer。该架构分别对节点和边进行嵌入，并通过一个修改过的注意力机制聚合它们。</p></li><li><p><u>Do Transformers Really Perform Badly for Graph Representation</u> (Ying et al, 2021) 介绍了微软的 <u>Graphormer</u>，该模型在面世时赢得了 OGB 第一名。这个架构使用节点特征作为注意力的查询&#x2F;键&#x2F;值（Q&#x2F;K&#x2F;V），然后在注意力机制中把这些表示与中心性、空间和边编码信息通过求和的方式结合起来。</p></li></ul><p>应该更好的工作是 <u>Pure Transformers are Powerful Graph Learners</u> (Kim et al, 2022)，它引入了 TokenGT。这一方法把输入图表示为一个节点和边嵌入的序列（并用正交节点标识（orthonormal node identifiers）和可训练的类型标识（type identifiers）增强它），而不使用位置嵌入，最后把这个序列输入给 Tranformer 模型。超级简单，但很聪明！</p><p>稍有不同的是，<u>Recipe for a General, Powerful, Scalable Graph Transformer</u> (Rampášek et al, 2022) 引入的不是某个模型，而是一个框架，称为 GraphGPS。它允许把消息传递网络和线性（长程的）transformer 模型结合起来轻松地创建一个混合网络。这个框架还包含了不少工具，用于计算位置编码和结构编码（节点、图、边层面的）、特征增强、随机游走等等。</p><p>在图数据上使用 transformer 模型还是一个非常初生的领域，但是它看上去很有前途，因为它可以减轻 GNN 的一些限制，如扩展到更大&#x2F;更稠密的图，抑或是增加模型尺寸而不必担心过平滑问题。</p>]]></content>
      
      
      <categories>
          
          <category> 图神经网络 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CS224W系列 | (1)图机器学习介绍</title>
      <link href="/2025/02/09/%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"/>
      <url>/2025/02/09/%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="我们将要探索的"><a href="#我们将要探索的" class="headerlink" title="我们将要探索的"></a>我们将要探索的</h2><ul><li>节点嵌入方法: DeepWalk, Node2Vec</li><li>图神经网络: GCN, GraphSAGE, GAT…</li><li>图变换器</li><li>知识图谱与推理: TransE, BetaE</li><li>图生成模型: GraphRNN</li><li>三维图: 分子结构</li><li>大规模图处理</li><li>在生物医药、科学、技术领域的应用</li></ul><h2 id="我们经常遇到的"><a href="#我们经常遇到的" class="headerlink" title="我们经常遇到的"></a>我们经常遇到的</h2><p>一个典型例子就是异构图。<br>异构图定义为:<br>G &#x3D; (V, E, R, T)</p><p>其中:</p><ul><li>节点及节点类型 vi ∈ V</li><li>带关系类型的边 (vi, r, vj) ∈ E</li><li>节点类型映射函数 T(vi)</li><li>关系类型 r ∈ R</li><li>节点和边都可以具有属性&#x2F;特征</li></ul><p>简单的说，异构图是节点类型不同或关系类型不同的图。而这种结构在现实世界中非常常见,例如:</p><ul><li>社交网络中的用户、帖子、评论等不同类型的节点</li><li>知识图谱中的实体和关系</li><li>推荐系统中的用户、物品、类别等多种节点类型<br><img src="https://pic1.imgdb.cn/item/67a89fb0d0e0a243d4fd89da.png" alt="异构图"></li></ul><h2 id="选择合适的图表示方法"><a href="#选择合适的图表示方法" class="headerlink" title="选择合适的图表示方法"></a>选择合适的图表示方法</h2><p>选择一个合适的图表示方法决定着我们是否能够顺利使用网络，如果不能很好的表示出其原本具有的信息，那对其相关的训练也会产生误差。</p><h3 id="无向图-vs-有向图"><a href="#无向图-vs-有向图" class="headerlink" title="无向图 vs 有向图"></a>无向图 vs 有向图</h3><p>顾名思义，无向图的边没有方向，而有向图的边有方向。<br><img src="https://pic1.imgdb.cn/item/67a8a186d0e0a243d4fd89fd.png" alt="有向图与无向图"></p><h3 id="二分图"><a href="#二分图" class="headerlink" title="二分图"></a>二分图</h3><p>二分图是一种特殊的图结构，其节点可以被分为两个不相交的集合U和V，使得每条边都连接U中的一个节点到V中的一个节点。也就是说，U和V是独立的集合。</p><img src="https://pic1.imgdb.cn/item/67a8a242d0e0a243d4fd8a16.png" alt="二分图" style="width:40%; display:block; margin:0 auto;"><p>举个例子有:</p><ul><li>作者-论文关系(作者撰写的论文)</li><li>演员-电影关系(演员出演的电影) </li><li>用户-电影关系(用户评分的电影)</li><li>食谱-配料关系(食谱包含的配料)</li><li>“折叠”网络:<ul><li>作者合作网络</li><li>电影共同评分网络</li></ul></li></ul><h2 id="Graph-ML-图机器学习-的应用"><a href="#Graph-ML-图机器学习-的应用" class="headerlink" title="Graph ML(图机器学习)的应用"></a>Graph ML(图机器学习)的应用</h2><p><img src="https://pic1.imgdb.cn/item/67a8a48ad0e0a243d4fd8a8b.png" alt="图预测任务"></p><p>我们有三种不同种类的任务，包括节点级别的分类、边级别的分类、图级别的分类。</p><p><img src="https://pic1.imgdb.cn/item/67a8a525d0e0a243d4fd8aa8.png" alt="节点分类"></p><h3 id="节点预测"><a href="#节点预测" class="headerlink" title="节点预测"></a>节点预测</h3><p><img src="https://pic1.imgdb.cn/item/67a8a561d0e0a243d4fd8ab0.png" alt="节点分类"></p><p>如果给你如左图所示的图，你应该如何知道灰色节点的颜色？</p><p>这正是我们节点预测要做的。具体方法我们在后文会进行分析。<br><del>期不期待！</del></p><h4 id="节点级网络结构"><a href="#节点级网络结构" class="headerlink" title="节点级网络结构"></a>节点级网络结构</h4><p>在开始进行节点分类等任务之前,我们得先搞清楚节点在网络中的”身份特征”。就像认识一个人要从多个角度了解一样,我们也需要从三个维度来认识节点:</p><ol><li><p>节点度(Node degree)</p><ul><li>简单来说就是看这个节点有多少”朋友”</li><li>这是最基础但也是最重要的特征哦</li></ul></li><li><p>节点重要性与位置(Node importance &amp; position)</p><ul><li>通过数最短路径来看节点的”地位”</li><li>看看它离其他节点有多远</li><li>这就像在社交网络中,有些人总是消息灵通,因为他们处在”中心位置”</li></ul></li><li><p>节点周围的子结构(Substructures)</p><ul><li>观察节点周围的”朋友圈”是什么样的</li><li>这能告诉我们很多关于节点角色的信息</li></ul></li></ol><h4 id="Graphlets-图形子结构"><a href="#Graphlets-图形子结构" class="headerlink" title="Graphlets(图形子结构)"></a>Graphlets(图形子结构)</h4><p>说完了基本的节点特征,我们来看一个更有趣的概念 - Graphlets! 它可以帮我们更细致地描述节点周围的”朋友圈”结构。</p><p>那么什么是Graphlets呢？简单来说,它就是统计以某个节点为”根”的各种子图数量。这些子图的形状是固定的,我们把每种形状的出现次数都数出来,最后得到一个计数向量。<br><del>听起来有点抽象?别急,看例子就明白了!</del></p><p><img src="https://pic1.imgdb.cn/item/67a8a81ad0e0a243d4fd8b3e.png" alt="Graphlets示例"></p><p>让我们一起看这个例子:</p><ol><li>首先,右边展示了所有可能的3节点以内的graphlet类型(a,b,c)</li><li>左边是一个具体的图G,我们要分析红色节点u</li><li>下面就是我们找到的所有以u为根的graphlet实例</li></ol><p>最后我们得到一个向量[2,1,0,2],这个向量就描述了节点u周围的”朋友圈”结构特征。是不是感觉Graphlets其实就像是在数”朋友圈”里有多少种不同的小团体呢？</p><blockquote><p>向量[2,1,0,2]如何得到？:</p><ul><li>2个a节点类似</li><li>1个b节点类似</li><li>0个c节点类似</li><li>2个d节点类似</li></ul></blockquote><p>当然会有人好奇为什么c不是，不是也有三角形吗？但是我们的要求是很严格的，c要求G邻居两个节点不能相连，显然不符合要求。</p><h4 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h4><h5 id="蛋白质折叠预测"><a href="#蛋白质折叠预测" class="headerlink" title="蛋白质折叠预测"></a>蛋白质折叠预测</h5><p><img src="https://pic1.imgdb.cn/item/67a8aae8d0e0a243d4fd8bc7.png" alt="蛋白质预测"></p><p>在生物学研究中,预测蛋白质的3D结构是一个重要且具有挑战性的问题。我们可以把蛋白质看作一个图结构:</p><ol><li><p>节点表示氨基酸</p><ul><li>每个氨基酸都有其特定的性质</li><li>它们的空间位置决定了蛋白质的功能</li></ul></li><li><p>边表示氨基酸之间的相互作用</p><ul><li>可以是化学键</li><li>也可以是空间上的近邻关系</li></ul></li></ol><p>通过图机器学习方法,我们可以:</p><ul><li>预测每个节点(氨基酸)的3D坐标</li><li>考虑节点之间的关系和整体结构</li><li>最终得到蛋白质的完整3D构型</li></ul><p>就像上图展示的那样,通过这种方法,我们可以仅仅基于氨基酸序列就可以预测出蛋白质的3D结构。</p><h5 id="药物研发"><a href="#药物研发" class="headerlink" title="药物研发"></a>药物研发</h5><p>在药物研发领域,图机器学习也发挥着重要作用。我们可以将分子结构表示为图:</p><ol><li><p>节点表示原子</p><ul><li>不同类型的原子有不同的化学性质</li><li>原子的排列方式决定了分子的功能</li></ul></li><li><p>边表示化学键</p><ul><li>单键、双键或三键</li><li>决定了分子的稳定性和反应性</li></ul></li></ol><p>通过图机器学习,我们可以:</p><ul><li>预测新分子的生物活性</li><li>设计具有特定性质的新药物</li><li>优化现有药物的结构</li></ul><h3 id="边预测"><a href="#边预测" class="headerlink" title="边预测"></a>边预测</h3><p><img src="https://pic1.imgdb.cn/item/67a8ae3dd0e0a243d4fd8c60.png" alt="边预测"></p><p>边预测任务是什么？<br>是要去判断两个节点之间是否存在边。<br>在测试时，我们会生成k个最有可能连接的边，根据概率排序。然后通过计算预测边和真实边之间差异来反向传播以训练模型。</p><p>当然还有一种方法时直接假装已有边不知道是不是有，然后让模型去预测。这就是自监督学习方法了。（比如假装不知道BD之间有没有边，然后让模型去预测。然后如果预测出来有那就对了。）</p><h4 id="两种边预测形式"><a href="#两种边预测形式" class="headerlink" title="两种边预测形式:"></a>两种边预测形式:</h4><ol><li><p>随机缺失的边预测</p><ul><li>从图中随机移除一些边</li><li>让模型预测这些被移除的边</li><li>这种方法适合静态图的链接预测</li></ul></li><li><p>时序边预测</p><ul><li>给定t0时刻之前的图结构</li><li>预测t1时刻可能出现的新边</li><li>输出一个排序后的边列表</li><li>通过与实际出现的边对比来评估</li></ul><p> 评估方法:</p><ul><li>计算在时间区间[t1,t1’]内出现的新边数量n</li><li>取预测边列表中的前n个</li><li>计算正确预测的边的数量</li><li>评估模型的预测准确率</li></ul></li></ol><p>这种预测任务的实际应用场景:</p><ul><li>社交网络中的好友推荐</li><li>知识图谱中的关系预测</li><li>蛋白质相互作用网络中的新连接发现</li></ul><h4 id="几个例子-1"><a href="#几个例子-1" class="headerlink" title="几个例子"></a>几个例子</h4><h5 id="推荐系统中的边预测"><a href="#推荐系统中的边预测" class="headerlink" title="推荐系统中的边预测"></a>推荐系统中的边预测</h5><p><img src="https://pic1.imgdb.cn/item/67a8b035d0e0a243d4fd8ceb.png" alt="推荐系统"></p><p>推荐系统是边预测的一个典型应用。在这种场景下:</p><ul><li><p>节点包括:</p><ul><li>用户节点(Users)</li><li>物品节点(Items,如电影、商品、音乐等)</li></ul></li><li><p>边表示:</p><ul><li>用户与物品之间的交互(如观看、购买、收听等)</li><li>这些交互形成了用户-物品二分图</li></ul></li><li><p>预测任务:</p><ul><li>根据已有的用户-物品交互记录</li><li>预测用户可能感兴趣的新物品</li><li>生成”猜你喜欢”的推荐列表</li></ul></li></ul><p>这种预测的特点是:</p><ul><li><p>利用图的结构特征捕捉用户兴趣</p></li><li><p>可以发现相似用户的共同偏好</p></li><li><p>帮助解决冷启动问题</p><blockquote><p>冷启动问题指的是系统对新用户或新物品缺乏历史交互数据,难以做出准确推荐。通过图结构,可以利用相似用户或物品的关系来缓解这个问题。</p></blockquote></li></ul><p>比如在上图所示的例子中:</p><ul><li>如果多个用户都购买了某些相似的商品</li><li>系统就可以推断出这些用户可能有相似的兴趣</li><li>进而推荐他们可能喜欢但尚未购买的其他商品</li></ul><h6 id="PinSage推荐系统"><a href="#PinSage推荐系统" class="headerlink" title="PinSage推荐系统"></a>PinSage推荐系统</h6><p><img src="https://pic1.imgdb.cn/item/67a8b0d6d0e0a243d4fd8d6b.png" alt="PinSage"></p><p>以Pinterest的PinSage系统为例:</p><ul><li><p>预测任务:</p><ul><li>向用户推荐相关的图片内容(Pin)</li><li>学习节点的嵌入表示</li><li>预测两个节点是否相关</li></ul></li><li><p>系统特点:</p><ul><li>将每个Pin建模为图中的一个节点</li><li>通过图神经网络学习节点的向量表示</li><li>利用节点之间的距离度量相似度</li><li>推荐距离较近的相关内容</li></ul></li><li><p>具体实现:</p><ul><li>如果用户对蛋糕图片感兴趣</li><li>系统会分析该图片的嵌入向量</li><li>找到向量空间中距离较近的其他内容</li><li>可能推荐相似的甜点图片</li><li>但不会推荐风格迥异的商品(如运动服)</li></ul></li></ul><p>这种基于图神经网络的推荐方法:</p><ul><li>能够自动学习内容之间的关联</li><li>捕捉用户兴趣的细微差异</li><li>生成更精准的个性化推荐</li></ul><h6 id="生物医学图链接预测"><a href="#生物医学图链接预测" class="headerlink" title="生物医学图链接预测"></a>生物医学图链接预测</h6><p><img src="https://pic1.imgdb.cn/item/67a8b184d0e0a243d4fd900c.png" alt="生物医学图"></p><p>在生物医学领域:</p><ul><li><p>预测任务:</p><ul><li>预测药物与蛋白质的相互作用</li><li>发现潜在的药物副作用</li><li>分析多种药物联合使用的影响</li></ul></li><li><p>图的构建:</p><ul><li>节点包括药物和蛋白质</li><li>边表示它们之间的相互作用</li><li>不同类型的边代表不同作用关系<ul><li>药物-蛋白质相互作用</li><li>蛋白质-蛋白质相互作用</li><li>药物的副作用</li></ul></li></ul></li><li><p>具体应用:</p><ul><li>预测新药与已知蛋白质的作用</li><li>分析药物组合可能产生的副作用</li><li>发现新的治疗用途</li><li>优化药物开发流程</li></ul></li></ul><h3 id="图级别任务"><a href="#图级别任务" class="headerlink" title="图级别任务"></a>图级别任务</h3><p>我们想要根据一个整图或子图结果进行预测。</p><p>例如交通网络预测。<br><img src="https://pic1.imgdb.cn/item/67a8b232d0e0a243d4fd92b3.png" alt="交通网络预测"></p><p>在交通网络预测中:</p><p><img src="https://pic1.imgdb.cn/item/67a8b2a8d0e0a243d4fd9438.png" alt="交通"></p><ul><li>预测任务:<ul><li>预测车辆到达时间</li><li>分析交通流量变化</li><li>优化路线规划</li></ul></li></ul><p><img src="https://pic1.imgdb.cn/item/67a8b2ebd0e0a243d4fd9443.png" alt="图的构建"></p><ul><li><p>图的构建:</p><ul><li>节点表示路口和地点</li><li>边表示道路连接</li><li>边的属性包含:<ul><li>道路长度</li><li>实时车流量</li><li>历史通行时间</li></ul></li></ul></li><li><p>具体应用:</p><ul><li>Google Maps导航<ul><li>收集匿名出行数据</li><li>通过图神经网络训练</li><li>预测最优路线和到达时间</li><li>为用户提供实时导航建议</li></ul></li><li>交通管理<ul><li>预测拥堵路段</li><li>优化信号灯配时</li><li>分散交通流量</li></ul></li></ul></li></ul><p>这种基于图神经网络的交通预测方法:</p><ul><li>能够处理复杂的路网结构</li><li>考虑多个影响因素</li><li>提供准确的实时预测</li></ul><h3 id="分子图预测"><a href="#分子图预测" class="headerlink" title="分子图预测"></a>分子图预测</h3><p>在药物发现和分子设计中,分子可以表示为图结构:</p><ul><li>节点表示原子</li><li>边表示化学键</li><li>节点和边的属性包含:<ul><li>原子类型</li><li>化学键类型</li><li>电荷分布</li><li>空间构型</li></ul></li></ul><p>例如抗生素分子的图表示:</p><ul><li>每个原子(C、H、O、N等)是一个节点</li><li>化学键(单键、双键等)是边</li><li>通过图神经网络可以:<ul><li>预测分子性质</li><li>分析药物活性</li><li>设计新型分子</li></ul></li></ul><p>具体应用包括:</p><ul><li>药物筛选<ul><li>预测候选分子的活性</li><li>评估毒性和副作用</li><li>优化分子结构</li></ul></li><li>材料设计<ul><li>预测材料性质</li><li>设计新型材料</li><li>优化合成路线</li></ul></li></ul><p>这种基于图的分子表示方法能够:</p><ul><li>保留分子的结构信息</li><li>捕捉原子间的相互作用</li><li>支持端到端的分子设计</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://pic1.imgdb.cn/item/67a8a48ad0e0a243d4fd8a8b.png" alt="总结"></p><p>本文我们主要介绍了图机器学习在节点预测、边预测、图级别任务和分子图预测中的应用。</p><p>接下来我们将对具体实现进行讨论。</p>]]></content>
      
      
      <categories>
          
          <category> 图神经网络 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CS224W系列 | 引言</title>
      <link href="/2025/02/09/CS224W%E8%A7%A3%E9%A2%98/"/>
      <url>/2025/02/09/CS224W%E8%A7%A3%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="这里有什么"><a href="#这里有什么" class="headerlink" title="这里有什么"></a>这里有什么</h2><p>本系列基于斯坦福大学CS224W课程，包含知识点与colab作业。</p><p><strong>本系列内容由亚里士原创！</strong></p><h2 id="官网课程链接"><a href="#官网课程链接" class="headerlink" title="官网课程链接"></a>官网课程链接</h2><p><a href="https://web.stanford.edu/class/cs224w/">CS224W: Machine Learning with Graphs</a></p><blockquote><p>我们永远在路上。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 图神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>向上</title>
      <link href="/2025/01/21/%E5%90%91%E4%B8%8A/"/>
      <url>/2025/01/21/%E5%90%91%E4%B8%8A/</url>
      
        <content type="html"><![CDATA[<p>小芷掰掰手指，心里数了数，自己已经15天没有成功过了。</p><p>太阳扑向操场上女孩子大大的眼，似乎想要把她吞进那红得发紫的日冕层。小芷不服，硬是睁开眼，和它对视。眼睛发酸，不听指令自己锁掉了眼皮。</p><p>“我是怎么了？”小芷不知道。可是她不知道别人可更不知道了。谁会去关心一个闲来无事就躺操场的、安安静静的孩子呢。</p><p>小芷光知道，自己苦苦练半年的排球，却被只练了两个月的好朋友轻轻打败；小芷光知道，自己努力了一学期学的课，却给了她一个不堪入目的成绩；小芷光知道，自己精心写出的歌，却在互联网上无人问津，还被顺带骂了几句。</p><p>小芷捋了捋自己的头发，乱的。</p><p>小芷揉了揉自己的眼睛，酸的。</p><p>小芷擦了擦自己的小鞋子，脏的。</p><p>“幸好妈妈看不到我，不然又要骂我了！”小芷庆幸。“这么不爱干净，一心都在你那奇奇怪怪又多样的爱好上！”母亲熟悉的话冲向小女孩，她赶忙摇头，“不要！不要！”小芷强迫自己不再回忆。自己也已经十七岁了诶。</p><p>操场上的脚步声错综复杂，扰乱着小女孩的思绪。“我也好久没跑了，天天来操场，却不跑步，上次跑步还是一个月前了诶。”小芷侧过身，背对太阳。太阳和小女孩做出了一个梯形的阴影。</p><p>“要不，跑一下？”小芷试探着问自己。</p><p>冲上操场，小芷一直在加速。理智告诉她不能一直这么冲刺，不然跑不了多久。“哼，很快就累了才好，我才不想跑呢！”小芷哼哼。小芷还挖出口袋里的耳机盒，边跑边打开手机，开了音乐。她继续加速，在永远循环无尽的操场上跑着。太阳和小女孩做出了一个圈……</p><p>当小女孩睁开眼睛的时候，天空已经是橙红色的了。太阳它只顾向下走，不曾回头。“嘻，竟然还碰到落日了耶！”小芷笑了。斗气的她才跑了一圈就已精疲力尽，扑向那太阳路过时捂的暖暖的草地。</p><p>小女孩看着太阳，得意，“我现在就这么看着你！眼睛都不带眨呢！”</p><p>可是太阳，只顾向下走。</p><p>小女孩回去的路上，落日把她的身子映照得好长好长，像是她走过的路。那影子一直向前延伸着，仿佛在指引她，所有的跌倒都是向上的伏笔。</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 短篇小说 </tag>
            
            <tag> 创作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>欢迎来到亚里士AI说</title>
      <link href="/2025/01/20/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E4%BA%9A%E9%87%8C%E5%A3%ABAI%E8%AF%B4/"/>
      <url>/2025/01/20/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E4%BA%9A%E9%87%8C%E5%A3%ABAI%E8%AF%B4/</url>
      
        <content type="html"><![CDATA[<h2 id="一页书，敬自己，敬过往，敬你"><a href="#一页书，敬自己，敬过往，敬你" class="headerlink" title="一页书，敬自己，敬过往，敬你"></a>一页书，敬自己，敬过往，敬你</h2><p>朋友，也许你正在网络搜索，无意间发现了我们的网站，也许你正处迷茫，不知道如何继续自己的生活。</p><blockquote><p>停下来，喝杯茶，我们一起聊聊，感悟人生，聊天聊地。</p></blockquote><hr><h2 id="关于这个网站"><a href="#关于这个网站" class="headerlink" title="关于这个网站"></a>关于这个网站</h2><p>这里有AI公众号 <strong>亚里士AI说</strong> 精神的传承，本人也是该公众号作者，我会在这里分享:</p><ul><li>莫名其妙的心理感悟</li><li>完整细腻的AI理论</li><li>突发奇想的文学创作</li><li>And so on…</li></ul><h2 id="作者是谁？"><a href="#作者是谁？" class="headerlink" title="作者是谁？"></a>作者是谁？</h2><p>正如网站显示的，作者是亚里士多章，目前正在大学攻读AI领域。</p><h3 id="大学所得成就"><a href="#大学所得成就" class="headerlink" title="大学所得成就"></a>大学所得成就</h3><ul><li>AI专业排名 <strong>Rank 1</strong></li><li>数学建模领域深耕，获奖有：<ul><li>“高教社杯”全国大学生数学建模竞赛 <strong>国家二等奖</strong></li><li>“电工杯”数学建模竞赛 <strong>国家一等奖</strong></li><li>本校数学建模竞赛一等奖</li></ul></li><li>还在探索的：<ul><li>asc竞赛</li><li>算法竞赛</li><li>数据挖掘</li><li>童话创作</li></ul></li></ul><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><blockquote><p>千里之行，始于足下</p></blockquote><p>这是一个起点，希望能在这里:</p><ul><li>见证我们的成长</li><li>记录我们的故事</li><li>存档我们的知识</li></ul><p>欢迎常来做客，让我们一起探索AI的奇妙世界~</p><hr><p>如果你也对AI感兴趣，欢迎:</p><ul><li>收藏本站常来看看</li><li>一起探讨技术与生活</li></ul><p>期待与你相遇！ ❤️</p>]]></content>
      
      
      <categories>
          
          <category> 随感 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 闲言碎语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监督学习模型：从输入到输出的映射</title>
      <link href="/2025/01/20/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
      <url>/2025/01/20/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="监督学习模型的定义"><a href="#监督学习模型的定义" class="headerlink" title="监督学习模型的定义"></a>监督学习模型的定义</h2><p>监督学习模型就是将一个或多个输入转化为一个或多个输出的方式。比如，我们可以将某部二手丰田普锐斯的车龄和行驶里程作为输入，预估的车辆价格则是输出。</p><p>相当于改变参数，然后可以形成一个正确函数进行预测。</p><div class="note success flat"><p>举例：easy-tensorflow中实现的神经网络训练底层就是这个原理，结果一定数量训练将随机生成的梯度往正确梯度上改变，最后实现正确预测。</p></div><h2 id="生成模型与判别模型的区别"><a href="#生成模型与判别模型的区别" class="headerlink" title="生成模型与判别模型的区别"></a>生成模型与判别模型的区别</h2><details><summary>生成 (Generative) 模型与判别 (Discriminative) 模型</summary><ul><li>判别模型就像一个熟练的图片分类员，看到图片就能快速判断</li><li>生成模型就像一个画家，了解狗的特征，可以画出各种狗的图片</li></ul><p>模型 <em>y</em> &#x3D; <em>f</em>[<em>x, ϕ</em>] 属于判别模型。这类模型基于实际测量的数据 x 来预测输出 y。另一种方法是构建生成模型 <em>x</em> &#x3D; <em>g</em>[<em>y, ϕ</em>]，在这种模型中，实际测量的数据 x 被看作是输出 y 的函数。</p><p>虽然生成模型的缺点是它们不直接预测 y，但它们的优势在于能够融入关于数据生成方式的先验知识。比如，如果我们要预测图像 x 中汽车的三维位置和方向 y，我们可以在函数 <em>x</em> &#x3D; <em>g</em>[<em>y, ϕ</em>] 中加入关于汽车形状、三维几何和光传输的知识。</p><p>尽管这听起来是个好主意，但实际上，在现代机器学习中，判别模型更为主流。这是因为在生成模型中利用先验知识所带来的优势通常不及利用大量训练数据来学习灵活的判别模型所获得的优势。</p></details><h2 id="损失函数与成本函数"><a href="#损失函数与成本函数" class="headerlink" title="损失函数与成本函数"></a>损失函数与成本函数</h2><details><summary>损失函数（Loss Function）与成本函数（Cost Function）</summary><p>在机器学习领域，”损失函数”和”成本函数”这两个术语通常可以互换使用。但更准确地说：</p><ul><li>损失函数是指与单个数据点相关的具体项（例如，方程 2.5 中每个平方项）</li><li>成本函数是指需要被最小化的整体量（即方程 2.5 中的整个右侧部分）</li></ul><p>成本函数可能还包含与单个数据点无关的其他项。更广义上，目标函数指的是任何需要最大化或最小化的函数。</p></details><h2 id="思考题与解答"><a href="#思考题与解答" class="headerlink" title="思考题与解答"></a>思考题与解答</h2><details><summary>例题</summary><p><strong>问题 2.1</strong> 为了在损失函数（方程 2.5）上实现”downhill”，我们需要计算它对参数 <em>ϕ</em>0 和 <em>ϕ</em>1 的梯度。请计算出这两个参数的梯度值 <em>∂L</em>&#x2F;<em>∂ϕ</em>0 和 <em>∂L</em>&#x2F;<em>∂ϕ</em>1 的具体表达式。</p><p><strong>问题 2.2</strong> 请证明我们可以通过将问题 2.1 中的导数设置为零，然后求解 <em>ϕ</em>0 和 <em>ϕ</em>1，以闭合形式找到损失函数的最小值。</p><p><strong>问题 2.3</strong> 考虑将线性回归改造为生成模型，形式为 <em>x</em> &#x3D; <em>g</em>[<em>y, ϕ</em>] &#x3D; <em>ϕ</em>0 + <em>ϕ</em>1<em>y</em>。请问这种情况下的新损失函数是什么？</p></details><details><summary>解答</summary><h3 id="问题-2-1-解答"><a href="#问题-2-1-解答" class="headerlink" title="问题 2.1 解答"></a>问题 2.1 解答</h3><p>损失函数L定义为：<br>L &#x3D; ∑(yi - (φ0 + φ1xi))²</p><ol><li><p>计算∂L&#x2F;∂φ0：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">∂L/∂φ0 = ∂/∂φ0 [∑(yi - (φ0 + φ1xi))²]</span><br><span class="line">= ∑ 2(yi - (φ0 + φ1xi)) * (-1)</span><br><span class="line">= -2∑(yi - φ0 - φ1xi)</span><br></pre></td></tr></table></figure></li><li><p>计算∂L&#x2F;∂φ1：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">∂L/∂φ1 = ∂/∂φ1 [∑(yi - (φ0 + φ1xi))²]</span><br><span class="line">= ∑ 2(yi - (φ0 + φ1xi)) * (-xi)</span><br><span class="line">= -2∑(yi - φ0 - φ1xi)xi</span><br></pre></td></tr></table></figure></li></ol><h3 id="问题-2-2-解答"><a href="#问题-2-2-解答" class="headerlink" title="问题 2.2 解答"></a>问题 2.2 解答</h3><p>要找到损失函数的最小值，我们将导数设为零并解方程：</p><p>φ1 &#x3D; (n∑xiyi - ∑xi∑yi) &#x2F; (n∑xi² - (∑xi)²)<br>φ0 &#x3D; (∑yi - φ1∑xi) &#x2F; n</p><p>这种方法只适用于线性回归，因为它具有特殊的二次形式损失函数。</p><h3 id="问题-2-3-解答"><a href="#问题-2-3-解答" class="headerlink" title="问题 2.3 解答"></a>问题 2.3 解答</h3><ol><li>新的生成模型：x &#x3D; g[y, φ] &#x3D; φ0 + φ1y</li><li>新的损失函数：L &#x3D; ∑(xi - (φ0 + φ1yi))²</li><li>逆函数：y &#x3D; (x - φ0) &#x2F; φ1</li></ol><p>在线性回归的情况下，生成模型和判别模型会得到相同的拟合线，但在更复杂的模型中可能不适用。在实践中，判别模型通常更受欢迎，因为它们直接针对预测任务进行优化。</p></details>]]></content>
      
      
      <categories>
          
          <category> 机器学习基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
